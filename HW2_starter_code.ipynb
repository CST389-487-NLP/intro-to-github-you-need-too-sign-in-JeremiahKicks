{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CST389-487-NLP/intro-to-github-you-need-too-sign-in-JeremiahKicks/blob/main/HW2_starter_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b38adc0c"
      },
      "source": [
        "# <font color=\"magenta\">Programming assignment 1</font>\n",
        "<font color=\"blue\"> 7pts - assignment is not optional, but counts as bonus</font>\n",
        "\n",
        "The assignment is structured as follows: the beginning cells are the starter code that you must read and understand. Then you'll see the red bold title \"Assignment\". The markdown cell that follows the title describes partial starter code in the next cell. This next cell has 'fill-in-blancks' parts that are your repsoncibility. Then the rest of the assignment is described in the following markdown cell\n",
        "\n",
        "\n",
        "## Training IMDB sentiment classification model\n",
        "Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf5edbda"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install torchinfo\n",
        "!pip install torch==2.3.0 torchtext==0.18.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary modules"
      ],
      "metadata": {
        "id": "aHHRAjOtKACa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional\n",
        "from torch import optim\n",
        "from torch.utils import data\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "1pVG-C_EPLQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell inmports datasets library from Hugging face. To be able to run it you need to\n",
        "  - Create a Hugging Face account and log into it,\n",
        "  - Navigate to Settings: Click on your profile picture in the top-right corner, then select Settings from the dropdown menu,\n",
        "  - Go to Access Tokens: In the settings menu on the left side, click on the Access Tokens tab,\n",
        "  - Generate a New Token: Click the New Token button\n",
        "  - Configure the Token:\n",
        "    1.   Name your token\n",
        "    2.    Choose the access level. In this cases, a \"read\" token (read-only access to public and granted-access models/datasets) is sufficient and recommended for security (we do not need to push results onto Hugging face\n",
        "  - Click the Generate a token (or Create token) button. The full token value will be displayed immediately. Copy it, as it will not be shown in full again for security reasons\n",
        "Next you would need to store the token in your colab security locker\n",
        "  - Open Secrets: In your Google Colab notebook, click the key icon (ðŸ”‘) in the left-hand sidebar,\n",
        "  - Click the + Add new secret button,\n",
        "  - In the \"Name\" field, type `HF_TOKEN` (this is the standard name used by Hugging Face libraries but if you want to you can use your own),\n",
        "  - Paste your Hugging Face API token into the \"Value\" field\n",
        "  - Toggle the Notebook access switch to the right (it will turn blue) to allow your current notebook to use this secret\n",
        "For this notebook nothing else is needed - when you run code in the next cell you will be asked to allow hugging face access to notebook. Say 'yes' and it'll run. In the future you may need to use api to access Hugging face. In this case just use this code:\n",
        "\n",
        "```python\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Retrieve the secret\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "```"
      ],
      "metadata": {
        "id": "ctnBdWLuKXrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Load the dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# 2. Access splits\n",
        "train_data = dataset['train']\n",
        "test_data = dataset['test']\n",
        "\n",
        "# 3. View an example: {'text': 'I love sci-fi...', 'label': 1}\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "id": "5KvGHOEbCcsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57954931"
      },
      "source": [
        "##  Vocabulary Building\n",
        "Next cell imports two crucial functions from the torchtext library for natural language processing:\n",
        "  - `get_tokenizer`: This function is used to create a tokenizer. A tokenizer is responsible for breaking down raw text into smaller units called 'tokens' (usually words or subwords). For example, it can turn a sentence like 'Hello world!' into ['hello', 'world', '!'].\n",
        "  - `build_vocab_from_iterator`: This function constructs a vocabulary from an iterator that yields sequences of tokens. A vocabulary maps each unique token to a unique numerical index, which is essential for converting text into a numerical format that a neural network can process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "metadata": {
        "id": "7lpCldoPRK6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell is we prepare the text data for a neural network by converting raw text into a numerical representation. It performs two main steps:\n",
        "- Define the tokenizer: The line tokenizer = `get_tokenizer(\"basic_english\", language=\"en\")` creates a tokenizer instance using torchtext's get_tokenizer function. This tokenizer is responsible for splitting raw sentences into individual words or tokens,\n",
        "- Build the vocabulary: This is the core part where a mapping from tokens to numerical indices is created.\n",
        "  - The 'yield_tokens' function is a helper generator. It iterates through the train_data (which contains dictionaries with a 'text' key) and applies the tokenizer to each text, yielding a stream of tokens. This stream is what `build_vocab_from_iterator` expects.\n",
        "  - `vocab = build_vocab_from_iterator(...)` constructs the vocabulary. It takes the token stream from `yield_tokens`, includes special tokens like <unk> (for unknown words not in the vocabulary) and <pad> (for padding sequences to the same length), and only includes words that appear at least `min_freq=2` times in the training data to reduce vocabulary size and noise.\n",
        "  - `vocab.set_default_index(vocab[\"<unk>\"])` ensures that any word encountered during inference that was not in the training vocabulary will be mapped to the index of the <unk> token.\n",
        "  - Finally, the print statements display information about the built vocabulary, such as its size and the indices of some specific words and special tokens, to verify its creation."
      ],
      "metadata": {
        "id": "jrgt_T8RPcbw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b849dcfe"
      },
      "source": [
        "# 1. Define the tokenizer\n",
        "# A basic English tokenizer is sufficient for the IMDB dataset\n",
        "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
        "print(\"Tokenizer initialized.\")\n",
        "\n",
        "# 2. Build the vocabulary\n",
        "# Define a generator function to yield tokens from the training data\n",
        "def yield_tokens(data_iter):\n",
        "    # data_iter is a list of dictionaries like {'text': '...', 'label': 0}\n",
        "    for item in data_iter:\n",
        "        yield tokenizer(item['text'])\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    yield_tokens(train_data), # Use the loaded train_data\n",
        "    specials=[\"<unk>\", \"<pad>\"], # Special tokens for unknown words and padding\n",
        "    min_freq=2 # Minimum frequency for a word to be included in the vocab\n",
        ")\n",
        "vocab.set_default_index(vocab[\"<unk>\"]) # Set default index for unknown words\n",
        "print(f\"Vocabulary built with {len(vocab)} unique tokens.\")\n",
        "\n",
        "# Display some vocabulary information for verification\n",
        "print(f\"<unk> token index: {vocab['<unk>']}\")\n",
        "print(f\"<pad> token index: {vocab['<pad>']}\")\n",
        "print(f\"Index of 'the': {vocab['the']}\")\n",
        "print(f\"Index of 'movie': {vocab['movie']}\")\n",
        "print(f\"Top 10 most frequent tokens: {vocab.get_itos()[:10]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ba0722c"
      },
      "source": [
        "This cell defines a custom `IMDBDataset` class, which is a crucial component for handling your data in PyTorch, especially for tasks like natural language processing. We do the following:\n",
        "- import Dataset from `torch.utils.data`, which is the base class for custom datasets in PyTorch.\n",
        "- Define text_pipeline. It is the lambda function that takes a raw text string, tokenizes it using the tokenizer (defined in a previous cell), and then converts those tokens into numerical indices using the vocab (also built in a previous cell). This transforms text into a sequence of numbers that a neural network can process.\n",
        "- Define `label_pipeline`. This lambda function is designed to convert string labels (like 'pos' or 'neg') into numerical labels (1 or 0). For IMDB dataset, labels are already be integers, so this pipeline in not used here, but it a standard preprocessing step, so I kept it here.\n",
        "\n",
        "The IMDBDataset class inherits from torch `utils.data.Dataset` and has three main methods:\n",
        "  - `__init__` initializes the dataset with the raw data list (e.g., train_data), and the `text_pipeline` and `label_pipeline` functions,\n",
        "  - `__len__` returns the total number of items in the dataset, which is essential for PyTorch's DataLoader,\n",
        "  - `__getitem__` is the core method. When you access an item by index (e.g., dataset[0]), it retrieves the text and label for that index, applies the text_pipeline to convert text to token IDs, converts the label to a torch.long tensor (suitable for loss functions like CrossEntropyLoss), and then returns the processed label and text tensor,\n",
        "  - `Instantiate Datasets` code creates two instances of IMDBDataset:\n",
        "    - train_dataset_custom using train_data.\n",
        "    - test_dataset_custom using test_data.\n",
        "The rest are print statements to show the size of the created datasets and to display a sample item (label and first few token IDs) from the training dataset, ensuring that the data is being processed correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d81ff29"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "# 3. Define the data processing pipeline (text to numerical tensor) and custom Dataset\n",
        "# (Moved from previous cell to be part of the dataset class for better encapsulation)\n",
        "\n",
        "# Define the text pipeline using the previously built vocab and tokenizer\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "# Define the label pipeline to convert 'pos'/'neg' to 1/0\n",
        "# This pipeline is not strictly necessary here as labels from datasets.load_dataset are already integers.\n",
        "label_pipeline = lambda x: 1 if x == 'pos' else 0\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data_list, text_pipeline, label_pipeline):\n",
        "        self.data_list = data_list\n",
        "        self.text_pipeline = text_pipeline\n",
        "        self.label_pipeline = label_pipeline\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # self.data_list[idx] returns a dictionary like {'text': 'review content', 'label': 0}\n",
        "        # Direct unpacking into `text, label` fails for dictionaries. Instead, access by key.\n",
        "        item = self.data_list[idx]\n",
        "        text = item['text']\n",
        "        label = item['label'] # label is an integer (0 or 1)\n",
        "\n",
        "        processed_text = torch.tensor(self.text_pipeline(text), dtype=torch.int64)\n",
        "        # For CrossEntropyLoss, labels should be of type torch.long\n",
        "        processed_label = torch.tensor(label, dtype=torch.long)\n",
        "        return processed_label, processed_text\n",
        "\n",
        "# Create instances of the custom dataset\n",
        "print(\"Creating IMDB training dataset...\")\n",
        "train_dataset_custom = IMDBDataset(train_data, text_pipeline, label_pipeline)\n",
        "print(f\"Training dataset size: {len(train_dataset_custom)}\")\n",
        "\n",
        "print(\"Creating IMDB test dataset...\")\n",
        "test_dataset_custom = IMDBDataset(test_data, text_pipeline, label_pipeline)\n",
        "print(f\"Test dataset size: {len(test_dataset_custom)}\")\n",
        "\n",
        "# Verify a sample from the custom dataset\n",
        "print(\"\\nSample from custom training dataset (label, token_ids):\")\n",
        "sample_label, sample_text_tensor = train_dataset_custom[0]\n",
        "print(f\"Label: {sample_label}\")\n",
        "print(f\"Text tensor (first 10 tokens): {sample_text_tensor[:10]}\")\n",
        "print(f\"Original text (first 50 chars): {train_data[0]['text'][:50]}...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09c56e35"
      },
      "source": [
        "The next cell creates the efficient data loaders for nn PyTorch model, dealing with variable-length text sequences.\n",
        "- `Import DataLoader` part starts with importing DataLoader `from torch.utils.data`, which is PyTorch's utility for iterating over datasets in batches.\n",
        "- `Define collate_batch function` prepairs batches of data for the `torch.nn.EmbeddingBag` layer, I use in the nn model. When DataLoader yields a batch, it passes a list of samples (label, text_tensor) to this function. Here's what `collate_batch` does:\n",
        "  - It iterates through each sample in the batch.\n",
        "  - It collects all labels into label_list.\n",
        "  - It collects all processed text tensors (which are already numerical token IDs from the IMDBDataset) into text_list.\n",
        "  - It builds an offsets list. EmbeddingBag requires a single concatenated tensor of all text tokens in the batch, along with offsets that indicate where each individual text sequence starts in that concatenated tensor. So, offsets are cumulative sums of the lengths of the text sequences.`label_list` is stacked into a single tensor. Offsets are converted into a `torch.tensor` and `cumsum` (cumulative sum) is applied to correctly generate the starting indices for each text within the concatenated text_list. All `text_list` tensors are concatenated into one large text_list tensor.\n",
        "  - It returns the batch of labels, the concatenated text_list, and the offsets.\n",
        "\n",
        "In Create DataLoaders section I set up the actual data loaders for training and testing:\n",
        "- `BATCH_SIZE` is defined as 64, meaning 64 samples will be processed together.\n",
        "- `device` is set to 'cuda' if a GPU is available, otherwise 'cpu'.\n",
        "- `train_loader_custom` is created using `train_dataset_custom`, set to `shuffle=True` for mini-batch SGD, and importantly, uses `collate_batch` function.\n",
        "- `test_loader_custom` is created similarly for the test set, but `shuffle=False` as we only do forward pass for evaluation.\n",
        "\n",
        "Finally, we iterate through one batch from the train_loader_custom to print the shapes and a few samples of the labels, text tensor, and offsets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f59629e"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 4. Define the collate_fn for DataLoader\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for _label, _text in batch:\n",
        "        label_list.append(_label)\n",
        "        processed_text = _text # _text is already a tensor from IMDBDataset\n",
        "        text_list.append(processed_text)\n",
        "        offsets.append(processed_text.size(0)) # Length of each text sequence\n",
        "\n",
        "    label_list = torch.stack(label_list) # Changed to torch.stack\n",
        "    # offsets.pop(0) is incorrect. cumsum should be applied to lengths before the first offset.\n",
        "    # The offsets for `torch.nn.EmbeddingBag` should be `[0, len(text1), len(text1)+len(text2), ...]`\n",
        "    # The first element is 0, and then each subsequent element is the cumulative sum of text lengths.\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "\n",
        "    # Concatenate all text tensors into a single tensor\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list, text_list, offsets\n",
        "\n",
        "# 5. Create DataLoaders\n",
        "BATCH_SIZE = 64\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Creating DataLoaders...\")\n",
        "train_loader_custom = DataLoader(\n",
        "    train_dataset_custom,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "test_loader_custom = DataLoader(\n",
        "    test_dataset_custom,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch\n",
        ")\n",
        "print(\"DataLoaders created.\")\n",
        "\n",
        "# Verify by iterating through one batch\n",
        "print(\"\\nVerifying a batch from train_loader_custom:\")\n",
        "for labels, text_tensor, offsets in train_loader_custom:\n",
        "    print(f\"Batch labels shape: {labels.shape}\")\n",
        "    print(f\"Batch text tensor shape: {text_tensor.shape}\")\n",
        "    print(f\"Batch offsets shape: {offsets.shape}\")\n",
        "    print(f\"First few labels: {labels[:5]}\")\n",
        "    print(f\"First few text token IDs: {text_tensor[:10]}\")\n",
        "    print(f\"First few offsets: {offsets[:5]}\")\n",
        "    break\n",
        "\n",
        "print(\"\\nVerification of data loading pipeline complete. ModuleNotFoundError for torchdata.datapipes should be resolved with this custom pipeline.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDlvgeuCajt6"
      },
      "source": [
        "### <font color=\"red\"><b>Assginment</b></font>\n",
        "<font color=\"blue\"> Assignment starts here and continues to the end of NB</font>\n",
        "\n",
        "The next cell must define the IMDBMLP (IMDB Multi-Layer Perceptron) model, which is a neural network designed for sentiment classification. Below I summarize the required structure and requirements.\n",
        " - `class IMDBMLP(nn.Module)` - class definition of the IMDBMLP class inherits from nn.Module, the base class for all neural network modules in PyTorch.\n",
        "- `__init__ method` is the constructor where all the layers of the network are defined:\n",
        "- `self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, sparse=False)` creates an embedding layer. `EmbeddingBag` is particularly efficient for text classification where you might have variable-length sequences. It takes the `vocab_size` (total number of unique words), `embedding_dim` (the size of the vector representation for each word), and `sparse=False` ensures dense gradients, which is generally better for optimizers like Adam.\n",
        "- `self.fc1 = ...` defines the first fully connected (linear) layer. It takes the output from the embedding layer (which is an `embedding_dim`-sized vector representing the entire text) and transforms it into hidden_dim1 features.\n",
        "- `self.gelu1 =...` is the hidden GELU (Gaussian Error Linear Unit) activation layer, which introduces non-linearity after the first linear layer.\n",
        "- `self.fc2 =...` is the second fully connected layer, further transforming features.\n",
        "- `self.gelu2 = nn.GELU()` another GELU activation layer.\n",
        "-`self.fc3 = ...` is the final linear layer, that creates logits, i.e.  outputs `num_class` values (2 in this case, for positive/negative sentiment).\n",
        "-`self.softmax = nn.Softmax(dim=1)` layer applies the Softmax function to the output of fc3 to convert the raw scores into probabilities for each class, ensuring they sum to 1.\n",
        "After defitinions of layers come methods:\n",
        "- `forward` method defines how data flows through the network during a forward pass:\n",
        "`embedded = self.embedding(text, offsets)`. The input text (token IDs) and offsets (indicating start positions of each text in a batch) are passed to the EmbeddingBag layer to get the text embeddings.\n",
        "- The embedded output then sequentially passes through fc1, gelu1, fc2, gelu2, and fc3. Finally, self.softmax(x) converts the raw logits into probability distribution over the classes.\n",
        "\n",
        "You must define the following model hyperparameters:\n",
        "-`VOCAB_SIZE` Determined by the `len(vocab)` from previous cells.\n",
        "-`EMBEDDING_DIM` is the dimensionality of the word embeddings (you can choose any value from 50 to 2024 but in your comments you must justify your choice).\n",
        "- `HIDDEN_DIM1, HIDDEN_DIM2` are the sizes (number of neurons) of the hidden layers in the MLP (again it is your choice but you should explain it).\n",
        "- `NUM_CLASSES` is 2 for binary sentiment classification.\n",
        "Next we instantiate the model `model_imdb_mlp = IMDBMLP(...)` - it creates an actual instance of your neural network. And then print model summary using `torchinfo.summary()` that provides a detailed overview of the model, including its layers, output shapes, number of parameters for each layer, and total parameters. The input_size and dtypes arguments are crucial here to simulate the input shape that the model expects for text (a 1D tensor of token IDs) and offsets (a 1D tensor of start indices) for a sample batch.\n",
        "\n",
        "<font color=\"red\">you must fill in the dots in the next section</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80e42719"
      },
      "source": [
        "import torchinfo\n",
        "\n",
        "class IMDBMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim1, hidden_dim2, num_class):\n",
        "        super(IMDBMLP, self).__init__()\n",
        "        # Set sparse=False to ensure dense gradients for compatibility with Adam optimizer\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, sparse=False)\n",
        "        self.fc1 = nn.Linear(...(?))\n",
        "        self.gelu1 = nn.GELU()\n",
        "        self.fc2 = nn.Linear(...(?))\n",
        "        self.gelu2 = nn.GELU()\n",
        "        self.fc3 = nn.Linear(...,(?))\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "          ...(?)\n",
        "        return self.softmax(x)\n",
        "\n",
        "# Define model parameters\n",
        "VOCAB_SIZE = len(vocab) # vocab is from previous cells\n",
        "EMBEDDING_DIM = ...(?)\n",
        "HIDDEN_DIM1 = ...(?)\n",
        "HIDDEN_DIM2 = ...(?)\n",
        "NUM_CLASSES = ...(?)\n",
        "\n",
        "# Instantiate the model\n",
        "model_imdb_mlp = IMDBMLP(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM1, HIDDEN_DIM2, NUM_CLASSES)\n",
        "\n",
        "# Print model summary\n",
        "print(\"IMDBMLP Model Summary:\")\n",
        "# Correct input_size: text should be 1D, offsets should be 1D\n",
        "# Example: one review with 10 tokens (total 10 tokens for batch size 1)\n",
        "torchinfo.summary(model_imdb_mlp, input_size=[(10,), (1,)], dtypes=[torch.int64, torch.int64])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6719547"
      },
      "source": [
        "### The rest of the assignment\n",
        "In the following cells:\n",
        "* the model should be reinstantiated with\n",
        "apropriate values (in case ou just did memo in previous cell),\n",
        "* correct loss function should be chosen,\n",
        "* optimizer should be added,\n",
        "* number of training epochs should be defined,\n",
        "* training should be executed.\n",
        "* You must compute running loss and accuracy and output it during training.\n",
        "Once training is finished you must\n",
        "* evaluate your model on the test set,\n",
        "* output test lossand test accuarcy.\n",
        "<font color=\"blue\"><b>The code should have extensive description in markdown cells accompanying code. I will severly penalize absence of comment cells.</b></font>\n",
        "\n",
        "For the remaining part of the assignment you can/should use/modify code in my Jupyter notebook for demonstration of nn in class that can be accessed in week 3 repository in Github classroom (and/or file in week 3 weekly lectures  on BB).    \n",
        "\n"
      ]
    }
  ]
}